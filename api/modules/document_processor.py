import requests
from io import BytesIO
from langchain_community.document_loaders import (
    PyPDFLoader,
    TextLoader,
    PyPDFium2Loader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from api.modules.chroma_indexer import save_to_chroma
import logging
import tempfile
import os


def load_pdf_with_fallback(file_path: str):
    """
    Intenta cargar un PDF con diferentes loaders hasta encontrar texto vÃ¡lido.
    Evita PDFs escaneados o vacÃ­os.
    """
    loaders = [
        ("PyPDFLoader", PyPDFLoader(file_path)),
        ("PyPDFium2Loader", PyPDFium2Loader(file_path)),
    ]

    for name, loader in loaders:
        try:
            docs = loader.load()
            total_text = sum(len(doc.page_content.strip()) for doc in docs)

            logging.info(
                f"ğŸ” Loader {name} -> {len(docs)} pÃ¡ginas, "
                f"{total_text} caracteres extraÃ­dos"
            )

            if total_text > 0:
                logging.info(f"âœ… Usando {name} para este PDF")
                return docs

        except Exception as e:
            logging.warning(f"âš ï¸ Error con {name}: {e}")

    raise Exception("âŒ No se pudo extraer texto del PDF con ningÃºn loader")


def process_file(file_url: str, client_id: str):
    """
    Descarga un archivo desde Supabase, lo procesa, divide en chunks
    y lo guarda en Chroma de forma aislada por cliente.
    """
    tmp_file_path = None

    try:
        logging.info(f"ğŸ“¥ Descargando archivo desde: {file_url}")
        response = requests.get(file_url)

        if response.status_code != 200:
            raise Exception(
                f"âŒ No se pudo descargar el archivo desde Supabase. "
                f"CÃ³digo: {response.status_code}"
            )

        content_type = response.headers.get("content-type", "")
        file_bytes = response.content

        # --------------------------------------------------
        # ğŸ“„ Carga del documento
        # --------------------------------------------------
        if ".pdf" in file_url.lower() or "pdf" in content_type.lower():
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                tmp_file.write(file_bytes)
                tmp_file_path = tmp_file.name

            docs = load_pdf_with_fallback(tmp_file_path)
        else:
            loader = TextLoader(
                file_path_or_file=BytesIO(file_bytes),
                encoding="utf-8"
            )
            docs = loader.load()

        logging.info(f"ğŸ“„ Documento cargado: {len(docs)} pÃ¡ginas/secciones")

        for i, doc in enumerate(docs[:5]):
            logging.info(
                f"PÃ¡gina {i + 1} -> {len(doc.page_content.strip())} caracteres"
            )

        # --------------------------------------------------
        # âœ‚ï¸ Chunking
        # --------------------------------------------------
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )

        chunks = splitter.split_documents(docs)
        logging.info(f"ğŸ§  Documento dividido en {len(chunks)} chunks")

        # --------------------------------------------------
        # ğŸ” FIX 2 â€” Blindaje multi-tenant (CRÃTICO)
        # --------------------------------------------------
        for chunk in chunks:
            chunk.metadata = chunk.metadata or {}
            chunk.metadata["client_id"] = client_id

        # --------------------------------------------------
        # ğŸ’¾ Guardar en Chroma (indexer intacto)
        # --------------------------------------------------
        save_to_chroma(chunks, client_id)

        return chunks

    except Exception as e:
        logging.exception(f"âŒ Error procesando el documento para {client_id}")
        raise e

    finally:
        if tmp_file_path and os.path.exists(tmp_file_path):
            os.remove(tmp_file_path)
            logging.info(f"ğŸ—‘ï¸ Archivo temporal eliminado: {tmp_file_path}")
